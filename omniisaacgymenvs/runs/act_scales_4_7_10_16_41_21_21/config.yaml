task_name: ${task.name}
experiment: act_scales_4_7_10_16_41_21_21
num_envs: ''
seed: 42
torch_deterministic: false
max_iterations: 300
physics_engine: physx
pipeline: gpu
sim_device: gpu
device_id: 0
rl_device: cuda:0
multi_gpu: false
num_threads: 4
solver_type: 1
test: false
checkpoint: ''
evaluation: false
headless: true
enable_livestream: false
mt_timeout: 300
enable_recording: false
recording_interval: 2000
recording_length: 100
recording_fps: 30
recording_dir: ''
wandb_activate: false
wandb_group: ''
wandb_name: ${train.params.config.name}
wandb_entity: ''
wandb_project: omniisaacgymenvs
kit_app: ''
warp: false
task:
  name: ForwarderPick
  physics_engine: ${..physics_engine}
  env:
    numEnvs: ${resolve_default:1024,${...num_envs}}
    envSpacing: 20
    episodeLength: 300
    enableDebugVis: false
    controlFrequencyInv: 3
    startPositionNoise: 0.25
    startRotationNoise: 0.785
    fwdPositionNoise: 0.0
    fwdRotationNoise: 0.0
    fwdDofNoise: 0.25
    aggregateMode: 3
    actionScale:
    - 4
    - 7
    - 10
    - 16
    - 41
    - 21
    - 21
    woodLiftScale: 3.5
    grappleToWoodDistScale: 100
    woodToUnloadingPointDistScale: 1000
    woodToTargetDistScale: 1
    kps:
    - 5000000.0
    - 5000000.0
    - 5000000.0
    - 5000000.0
    - 5000000.0
    - 5000000.0
    - 5000000.0
    kds:
    - 0
    - 0
    - 0
    - 0
    - 0
    - 0
    - 0
    force:
    - 100000.0
    - 100000.0
    - 100000.0
    - 100000.0
    - 100000.0
    - 100000.0
    - 100000.0
    asset:
      assetRoot: ../../assets
      assetFileNameFranka: urdf/forwarder_description/forwarder.urdf
    enableCameraSensors: false
  sim:
    dt: 0.03
    use_gpu_pipeline: ${eq:${...pipeline},"gpu"}
    gravity:
    - 0.0
    - 0.0
    - -9.81
    add_ground_plane: true
    use_flatcache: true
    enable_scene_query_support: false
    disable_contact_processing: false
    enable_cameras: false
    default_physics_material:
      static_friction: 0.1
      dynamic_friction: 0.1
      restitution: 0.0
    physx:
      use_gpu: ${eq:${....sim_device},"gpu"}
      worker_thread_count: ${....num_threads}
      solver_type: 1
      bounce_threshold_velocity: 0.2
      friction_offset_threshold: 0.04
      friction_correlation_distance: 0.025
      enable_sleeping: true
      enable_stabilization: true
      gpu_max_rigid_contact_count: 1048576
      gpu_max_rigid_patch_count: 33554432
      gpu_found_lost_pairs_capacity: 20971520
      gpu_found_lost_aggregate_pairs_capacity: 262144
      gpu_total_aggregate_pairs_capacity: 20971520
      gpu_max_soft_body_contacts: 1048576
      gpu_max_particle_contacts: 1048576
      gpu_heap_capacity: 33554432
      gpu_temp_buffer_capacity: 16777216
      gpu_max_num_partitions: 8
    forwarder:
      override_usd_defaults: true
      enable_self_collisions: false
      enable_gyroscopic_forces: true
      solver_position_iteration_count: 12
      solver_velocity_iteration_count: 1
      sleep_threshold: 0.005
      stabilization_threshold: 0.001
      density: -1
      max_depenetration_velocity: 10000.0
train:
  params:
    seed: ${...seed}
    algo:
      name: a2c_continuous
    model:
      name: continuous_a2c_logstd
    network:
      name: actor_critic
      separate: false
      space:
        continuous:
          mu_activation: None
          sigma_activation: None
          mu_init:
            name: default
          sigma_init:
            name: const_initializer
            val: 0
          fixed_sigma: true
      mlp:
        units:
        - 256
        - 256
        - 128
        activation: elu
        d2rl: false
        initializer:
          name: default
        regularizer:
          name: None
    load_checkpoint: ${if:${...checkpoint},True,False}
    load_path: ${...checkpoint}
    config:
      name: ${resolve_default:ForwarderPick,${....experiment}}
      full_experiment_name: ${.name}
      env_name: rlgpu
      multi_gpu: ${....multi_gpu}
      ppo: true
      mixed_precision: false
      normalize_input: true
      normalize_value: true
      value_bootstrap: true
      num_actors: ${....task.env.numEnvs}
      reward_shaper:
        scale_value: 1
      normalize_advantage: true
      gamma: 0.99
      tau: 0.95
      learning_rate: 0.005
      lr_schedule: adaptive
      schedule_type: standard
      kl_threshold: 0.008
      score_to_win: 11250000
      max_epochs: ${resolve_default:10000,${....max_iterations}}
      save_best_after: 200
      save_frequency: 100
      print_stats: true
      grad_norm: 1.0
      entropy_coef: 0.0
      truncate_grads: true
      e_clip: 0.2
      horizon_length: 32
      minibatch_size: 8192
      mini_epochs: 8
      critic_coef: 2
      clip_value: true
      seq_len: 4
      bounds_loss_coef: 0.0001
